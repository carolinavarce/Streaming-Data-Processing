# Procesamiento de Datos en Streaming

Un pipeline de ingestiÃ³n y procesamiento en tiempo real de eventos de usuario, orquestado con Apache Kafka y Spark Streaming, y monitorizado con Grafana.

---

## ğŸ¯ Objetivos

1. Simular generaciÃ³n de eventos (logs, clicks, transacciones).  
2. Ingestar los eventos en Kafka.  
3. Procesar en tiempo real con Spark Streaming (windowing, agregaciones).  
4. Almacenar resultados en PostgreSQL.  
5. Visualizar mÃ©tricas y alertas en Grafana.

---

## ğŸ§° Tech Stack

- **MensajerÃ­a**: Apache Kafka (Zookeeper + brokers)  
- **Procesamiento**: Apache Spark Streaming (PySpark)  
- **Lenguaje**: Python 3.8+  
- **Contenedores**: Docker & Docker Compose  
- **Base de datos**: PostgreSQL  
- **MonitorizaciÃ³n**: Grafana  
- **Logs / mÃ©tricas**: Prometheus (opcional)  

---

## ğŸ“ Arquitectura

\`\`\`text
[Generador de eventos] â†’
  â””â–¶ Kafka Topic â€œeventsâ€ â†’
      â””â–¶ Spark Streaming Job â†’
          â”œâ–¶ PostgreSQL (agg/results)  
          â””â–¶ MÃ©tricas â†’ Prometheus â†’ Grafana
\`\`\`

---

## ğŸ“‚ Estructura del repositorio

\`\`\`
streaming-pipeline/
â”œâ”€â”€ docker-compose.yml        # Kafka + Zookeeper + Postgres + Grafana
â”œâ”€â”€ data-generator/
â”‚   â””â”€â”€ producer.py           # Script Python que envÃ­a mensajes a Kafka
â”œâ”€â”€ spark-job/
â”‚   â”œâ”€â”€ streaming_app.py      # Job PySpark que consume, procesa y escribe
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ grafana/
â”‚   â””â”€â”€ dashboards.json       # Export de dashboard con grÃ¡ficas de latencia, throughput
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql              # CreaciÃ³n de tablas en Postgres
â”œâ”€â”€ README.md                 # Este documento
â””â”€â”€ LICENSE
\`\`\`

---

## ğŸ”§ Prerrequisitos

- Docker & DockerÂ Compose  
- Java 8+ (para Kafka)  
- PythonÂ 3.8+ (si ejecutas local scripts sin Docker)  

---

## ğŸš€ InstalaciÃ³n y puesta en marcha

1. **Clona el repo**  
   \`\`\`bash
   git clone https://github.com/tu-usuario/streaming-pipeline.git
   cd streaming-pipeline
   \`\`\`

2. **Levanta los servicios**  
   \`\`\`bash
   docker-compose up -d
   \`\`\`  
   Esto arranca:  
   - Zookeeper  
   - Kafka (broker)  
   - PostgreSQL (puerto 5432)  
   - Grafana (puerto 3000)

3. **Inicializa la BD**  
   \`\`\`bash
   docker exec -i streaming_postgres psql -U postgres < sql/init.sql
   \`\`\`

4. **Arranca el generador de eventos**  
   \`\`\`bash
   cd data-generator
   pip install confluent-kafka
   python producer.py
   \`\`\`

5. **Ejecuta el job de Spark**  
   \`\`\`bash
   cd spark-job
   pip install -r requirements.txt
   spark-submit --master local[2] streaming_app.py
   \`\`\`

6. **Importa el dashboard a Grafana**  
   - Entra en \`http://localhost:3000\` (admin/admin)  
   - â€œImportâ€ â†’ sube \`grafana/dashboards.json\`

---

## ğŸ“Š MonitorizaciÃ³n

- **Throughput** de mensajes procesados.  
- **Latencia** de procesamiento (tiempo entre ingestiÃ³n y guardado).  
- **Errores** (mensajes mal formados).  
- Configura alertas en Grafana (umbral de latencia o fallo de job).

---


## ğŸ“œ Licencia

Este proyecto usa la licencia MIT. VÃ©ase \`LICENSE\`.  
